{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9f80644-3261-4d7c-bb1a-c5ad649cf10b",
   "metadata": {},
   "source": [
    "# Herramientas Profesionales con Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74ae361-79ca-4bf1-954b-bb5a3d775b08",
   "metadata": {},
   "source": [
    "https://scikit-learn.org\n",
    "\n",
    "Por Que Scikit-Learn?\n",
    "\n",
    "- Curva de aprendizaje suave.\n",
    "\n",
    "- Una libreria muy versatil.\n",
    "\n",
    "- Comunidad de soporte.\n",
    "\n",
    "- Usado en produccion.\n",
    "\n",
    "- Integracion con librerias externas.\n",
    "\n",
    "Modulos:\n",
    "\n",
    "- Clasificacion\n",
    "\n",
    "- Regresion\n",
    "\n",
    "- Clustering\n",
    "\n",
    "- Preprocesamiento\n",
    "\n",
    "- Reduccion de la dimensionalidad\n",
    "\n",
    "- Seleccion del modelo\n",
    "\n",
    "Las herramientas se enfocan en :\n",
    "\n",
    "- Como nos ayuda en el procesamiento de datos.\n",
    "\n",
    "- Modelos empleados para resolver problemas.\n",
    "\n",
    "- Procesedimientos para optimizar los modelos.\n",
    "\n",
    "Como aprenden las maquinas?\n",
    "\n",
    "1. Aprendizaje supervisado\n",
    "\n",
    "2. Aprendizaje no supervisado\n",
    "\n",
    "3. Aprendizaje por refuerzos\n",
    "\n",
    "Problemas que podemos resolver:\n",
    "\n",
    "Limitaciones:\n",
    "\n",
    "- No es una herramienta de computer vision.\n",
    "\n",
    "- No se puede correr en GPUs.\n",
    "\n",
    "- No es una herramienta de estadistica avanzada.\n",
    "\n",
    "- No es muy flexible en temas de deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ded26d-9553-4a88-8c3a-e58e662f7ab9",
   "metadata": {},
   "source": [
    "## Introduccion a PCA\n",
    "\n",
    "Analisis de Componentes Principales\n",
    "\n",
    "Algunos Casos de usos:\n",
    "\n",
    "- Nuestro dataset tiene un numero alto de features\n",
    "\n",
    "- Hay una alta correlacion entre los features\n",
    "\n",
    "- Overfitting\n",
    "\n",
    "- Alto costo computacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed96b818-b0f7-4993-888c-a6b52e018b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077429f9-c929-4dae-8d91-c8ef8cd5dd8b",
   "metadata": {},
   "source": [
    "## Kernels\n",
    "\n",
    "Es una funcion matematica que toma mediciones que se comportan de manera no lineal y las proyecta en un espacio dimensional mas grande donde sean linealmente separables.\n",
    "\n",
    "- Lienales\n",
    "- Polinomiales\n",
    "- Gaussianos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da507b09-0933-4abd-8289-11dfc87ecdd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Regularizacion\n",
    "\n",
    "Consiste en disminuir la complejidad del modelo a traves de una penalizacion aplicada a sus variables mas irrelevantes\n",
    "\n",
    "Perdida en entrenamiento y validacion, que tan lejos se esta de los datos reales.\n",
    "\n",
    "\n",
    "Tipos de Regularizaciones\n",
    "\n",
    "- L1 Lasso\n",
    "\n",
    "- L2 Ridge\n",
    "\n",
    "- ElasticNet\n",
    "\n",
    "Recordando que :\n",
    "\n",
    "-    Ninguna de las dos es mejor que la otra para todos los casos.\n",
    "\n",
    "-    Lasso envía algunos coeficientes a cero permitiendo así seleccionar variables significativas para el modelo.\n",
    "\n",
    "-    Lasso funciona mejor si tenemos pocos predictores que influyen sobre el modelo.\n",
    "\n",
    "-    Ridge funciona mejor si es el caso contrario y tenemos una gran cantidad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c558e56-d62d-4153-a4f7-de3836eb4560",
   "metadata": {},
   "source": [
    "## Valores atipicos\n",
    "\n",
    "- Puden generar sesgos importantes en los modelos de ML\n",
    "\n",
    "- A veces tienen informacion relevante sobre la naturaleza de los datos\n",
    "\n",
    "- Deteccion temprano de fallos\n",
    "\n",
    "Como identificarlos, a traves de metodos estadisticos\n",
    "\n",
    "- Z-score, distancia respecto a la media\n",
    "\n",
    "- Tecnica de clustering como DBSCAN\n",
    "\n",
    "- Si q<Q1-1.5*IQR o q > Q3+1.5*IQR (IQR, rango intercuatilico)\n",
    "\n",
    "Los box plots nos ayudan a mirar como se distribuye una variable\n",
    "\n",
    "Regresion robusta en Scikit-Learn:\n",
    "\n",
    "RANSAC (Random Sample Consensus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd65ba5-dfe0-4240-9d29-f50d8da353c9",
   "metadata": {},
   "source": [
    "## Metodos de ensamble\n",
    "\n",
    "- Combinar diferentes metodos de ML con diferentes configuraciones y plicar un metodo para lograr un consenso\n",
    "\n",
    "- La diversidad es una muy buena opcion\n",
    "\n",
    "- Los metodos de ensamble se han destandop por ganar mucha competencia de ML\n",
    "\n",
    "\n",
    "Baggin (Bootstrap AGGregation):\n",
    "(Estrategia en paralelo)\n",
    "- Random Forest, convina varios arboles de descicion\n",
    "\n",
    "- Voting Classifiers/Regressors\n",
    "\n",
    "- Se puede aplicar sobre cualquier familia de ML\n",
    "\n",
    "(Estrategia en seria)\n",
    "\n",
    "Boosting: Impulsar / Propulsar\n",
    "\n",
    "Modelos de ensable:\n",
    "\n",
    "- AdaBoot\n",
    "\n",
    "- Gradient Tree Boosting\n",
    "\n",
    "- XGBoots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03496d0-81f0-4b6d-9f48-ac20ec831957",
   "metadata": {},
   "source": [
    "## Clustering (No supervizado)\n",
    "\n",
    "- No conocemos con antisipacion las etiquetas.\n",
    "\n",
    "- No sabemos nada de los datos\n",
    "\n",
    "- Identificar valores atipicos\n",
    "\n",
    "Dos casos de aplicacion\n",
    "\n",
    "- Cuando se sabe cuantos grupos se quieren\n",
    "    - k-means, spectral clustering\n",
    "\n",
    "- Cuando queremos que el algoritmo nos de los grupos:\n",
    "    - Meanshift, cluster herarquico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdf5533",
   "metadata": {},
   "source": [
    "## Validacion de nuestros modelos usando Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2431db7",
   "metadata": {},
   "source": [
    "- La ultima palabra la tienen los datos\n",
    "\n",
    "- Necesitamos mentalidad de testeo\n",
    "\n",
    "- Todos los modelos son malos, solamante que algunos resultan utiles.\n",
    "\n",
    "Tipos de Validacion cruzada:\n",
    "\n",
    "- Dividir nuestros datos en Entrenamiento / Prueba (Hold-Out). Prototipado rapido. No se tiene mucho conocimiento. No se requiere mucho poder de computo.\n",
    "\n",
    "- Usar validacion cruzada (K-Folds). Pleagar nuestros datos k veces, y en cada pliegue se utilizara diferentes partes de nuestro dataset. Recomendable en la mayoria de los casos. Se cuenta con un equipo suficiente para desarrollar ML. Se requiere la integracion con tecnicas de optimizacion parametrica. Se tiene mas tiempo para las pruebas.\n",
    "\n",
    "     - Optimizacion parametrica, optimizar los parametros dato un modelo escogido. Es costoso computacionalmente.\n",
    "     \n",
    "     - Tres enfoques:\n",
    "\n",
    "         - Manual, Escoger el modelo que se quiere ajustar. Buscar la documentacion. Identificar los posibles ajustes. Probar combinaciones una por una iterando a traves de listas.\n",
    "         \n",
    "         - Optimizacion por grilla de parametros GridSearchCV. Definir una o varias variables metricas que queremos optimizar. Identificar los posibles valores que pueden tener los parametros. Crear diccionarios de parametros. Usar cross validation. Entrenar el modelo e ir por un cafe.\n",
    "\n",
    "         - Optimizacion por busqueda Aleatoria, RandomizedSearchCV. Definir una o varias variables metricas que queramos optimizar. Identificar los rangos de valores que pueden tomar ciertos parametros. Crear un diccionario de rango de valores. Usar cross validation. Entrenar el modelo he ir por un cafe.\n",
    "\n",
    "- Validacion cruzada (LOOCV). Se requiere gran poder de computo. Se cuenta con pocos datos como para dividir por Training / Test. Se probaran todos los casos posibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151dd74d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
